# Reward Model training config for financial safety
name: reward_model_financial

model:
  micro_batch_size: 4
  global_batch_size: 128
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  
  # Reward Model specific params for preference learning
  reward_model:
    score_bias: 0.0
    score_scale: 1.0
    
  data:
    data_prefix:
      - 1.0
      - "data/preference_pairs.jsonl"
    num_workers: 4
    max_seq_length: 1024

  optim:
    name: distributed_fused_adam
    lr: 5e-6
    weight_decay: 0.1
